{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai71 import AI71\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from extract_pdf import extract_text_from_pdf, extract_text_from_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AI71(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(\"/home/space/Downloads/test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_texts = extract_text_from_pdfs([\"/home/space/Downloads/test.pdf\", \"/home/space/Downloads/test.pdf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a limit of around 5500 characters for the input data. Maybe can use streaming to input all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"tiiuae/falcon-180B-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a lesson planner.\"},\n",
    "        {\"role\": \"user\", \"content\": text[0:4800]},\n",
    "        {\"role\": \"user\", \"content\": \"Create 3 small lesson topics to teach the given material in json format giving a proper explanation on what the lesson should teach.\"},\n",
    "    ],\n",
    "    # max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat completion and completion are virtually the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_completion = client.completions.create(\n",
    "    model=\"tiiuae/falcon-180B-chat\",\n",
    "    prompt=f\"{text[0:5500]}, Create lesson topics to teach the given material.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"lesson_topics\": [\n",
      "    {\n",
      "      \"topic_name\": \"Calculus Overview\",\n",
      "      \"explanation\": \"This lesson will provide an overview of calculus, including its importance in machine learning, and cover topics such as vectorization, linear algebra, gradient estimation, gradient descent, partial derivatives, and the chain rule.\"\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"Statistics\",\n",
      "      \"explanation\": \"This lesson will focus on the importance of statistics in machine learning, including descriptive statistics such as mean, median, and mode, as well as variance and standard deviation. It will also cover hypothesis testing and p-values.\"\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"Gaussian Distribution\",\n",
      "      \"explanation\": \"This lesson will cover the Gaussian or normal distribution, including the central limit theorem, the mean and standard deviation, and confidence intervals. It will also cover hypothesis testing and p-values in the context of the Gaussian distribution.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculus Overview This lesson will provide an overview of calculus, including its importance in machine learning, and cover topics such as vectorization, linear algebra, gradient estimation, gradient descent, partial derivatives, and the chain rule.\n",
      "Statistics This lesson will focus on the importance of statistics in machine learning, including descriptive statistics such as mean, median, and mode, as well as variance and standard deviation. It will also cover hypothesis testing and p-values.\n",
      "Gaussian Distribution This lesson will cover the Gaussian or normal distribution, including the central limit theorem, the mean and standard deviation, and confidence intervals. It will also cover hypothesis testing and p-values in the context of the Gaussian distribution.\n"
     ]
    }
   ],
   "source": [
    "json_string = response.choices[0].message.content.replace(\"User:\", \"\")\n",
    "\n",
    "data = json.loads(json_string)\n",
    "# print(data)\n",
    "# Now you can access the data as a Python object\n",
    "lesson_topics = data['lesson_topics']\n",
    "\n",
    "for topic in lesson_topics:\n",
    "    print(topic['topic_name'], topic['explanation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_response = client.chat.completions.create(\n",
    "    model=\"tiiuae/falcon-180B-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a great teacher.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you create a detailed and extensive learning materials on the topic of 'Gradient Estimation and Gradient Descent'? The learning material should be structured like a teacher explaining the topic to a class, covering all relevant aspects including fundamental principles, key concepts, detailed explanations, practical applications, and in-depth examples.\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion_tokens=1650 prompt_tokens=80 total_tokens=1730\n"
     ]
    }
   ],
   "source": [
    "print(content_response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help you with that! Here's a detailed and extensive learning material on the topic of 'Gradient Estimation and Gradient Descent':\n",
      "\n",
      "Introduction:\n",
      "\n",
      "Gradient estimation and gradient descent are two fundamental concepts in machine learning and optimization. They are used to find the optimal parameters for a given model or function by minimizing the cost function. In this learning material, we will cover the fundamental principles, key concepts, detailed explanations, practical applications, and in-depth examples of gradient estimation and gradient descent.\n",
      "\n",
      "Fundamental Principles:\n",
      "\n",
      "The fundamental principle behind gradient estimation and gradient descent is to find the optimal parameters for a given model or function by minimizing the cost function. The cost function is a measure of how well the model or function fits the data. The goal is to find the parameters that minimize the cost function.\n",
      "\n",
      "Key Concepts:\n",
      "\n",
      "1. Gradient Estimation: Gradient estimation is the process of computing the gradient of the cost function with respect to the parameters. The gradient is a vector that points in the direction of steepest ascent. The gradient is used to update the parameters in the direction that reduces the cost function.\n",
      "\n",
      "2. Gradient Descent: Gradient descent is an optimization algorithm that uses the gradient to update the parameters. The algorithm starts with an initial set of parameters and iteratively updates them in the direction of steepest descent. The goal is to find the parameters that minimize the cost function.\n",
      "\n",
      "Detailed Explanations:\n",
      "\n",
      "1. Gradient Estimation: Gradient estimation is the process of computing the gradient of the cost function with respect to the parameters. The gradient is a vector that points in the direction of steepest ascent. The gradient is used to update the parameters in the direction that reduces the cost function.\n",
      "\n",
      "The gradient is computed using the chain rule. The chain rule states that the derivative of a function is the product of the derivative of the function with respect to its inputs. In the case of gradient estimation, the function is the cost function, and the inputs are the parameters.\n",
      "\n",
      "The gradient is computed as follows:\n",
      "\n",
      "âˆ‡f(Î¸) = [âˆ‚f(Î¸)/âˆ‚Î¸1, âˆ‚f(Î¸)/âˆ‚Î¸2, ..., âˆ‚f(Î¸)/âˆ‚Î¸n]\n",
      "\n",
      "where f(Î¸) is the cost function, and Î¸ is the vector of parameters.\n",
      "\n",
      "2. Gradient Descent: Gradient descent is an optimization algorithm that uses the gradient to update the parameters. The algorithm starts with an initial set of parameters and iteratively updates them in the direction of steepest descent. The goal is to find the parameters that minimize the cost function.\n",
      "\n",
      "The gradient descent algorithm is as follows:\n",
      "\n",
      "1. Initialize the parameters Î¸ to some initial value.\n",
      "\n",
      "2. Compute the gradient of the cost function with respect to the parameters.\n",
      "\n",
      "3. Update the parameters using the following formula:\n",
      "\n",
      "Î¸ = Î¸ - Î±âˆ‡f(Î¸)\n",
      "\n",
      "where Î± is the learning rate, and âˆ‡f(Î¸) is the gradient of the cost function with respect to the parameters.\n",
      "\n",
      "4. Repeat steps 2 and 3 until convergence or a maximum number of iterations is reached.\n",
      "\n",
      "Practical Applications:\n",
      "\n",
      "Gradient estimation and gradient descent are used in a wide range of applications, including:\n",
      "\n",
      "1. Linear Regression: Gradient descent is used to find the optimal parameters for a linear regression model.\n",
      "\n",
      "2. Logistic Regression: Gradient descent is used to find the optimal parameters for a logistic regression model.\n",
      "\n",
      "3. Neural Networks: Gradient descent is used to train neural networks by updating the weights and biases of the network.\n",
      "\n",
      "4. Support Vector Machines: Gradient descent is used to find the optimal parameters for a support vector machine.\n",
      "\n",
      "In-Depth Examples:\n",
      "\n",
      "1. Linear Regression:\n",
      "\n",
      "Suppose we have a dataset of 10 points:\n",
      "\n",
      "(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11)\n",
      "\n",
      "We want to fit a linear regression model to this data. The cost function for linear regression is:\n",
      "\n",
      "J(Î¸) = (1/m) * âˆ‘(h(x(i)) - y(i))^2\n",
      "\n",
      "where h(x(i)) = Î¸1 + Î¸2x(i), and m is the number of data points.\n",
      "\n",
      "We can compute the gradient of the cost function with respect to the parameters as follows:\n",
      "\n",
      "âˆ‡J(Î¸) = [2/m * âˆ‘(h(x(i)) - y(i)) * x(i), 2/m * âˆ‘(h(x(i)) - y(i))]\n",
      "\n",
      "We can use gradient descent to find the optimal parameters for the linear regression model. We start with an initial set of parameters Î¸ = [0, 0], and iteratively update them using the following formula:\n",
      "\n",
      "Î¸ = Î¸ - Î±âˆ‡J(Î¸)\n",
      "\n",
      "where Î± is the learning rate.\n",
      "\n",
      "We can choose a learning rate of 0.1 and run gradient descent for 100 iterations. The final parameters are:\n",
      "\n",
      "Î¸ = [0.5, 0.5]\n",
      "\n",
      "2. Logistic Regression:\n",
      "\n",
      "Suppose we have a dataset of 10 points:\n",
      "\n",
      "(1, 0), (2, 0), (3, 1), (4, 1), (5, 0), (6, 1), (7, 0), (8, 1), (9, 0), (10, 1)\n",
      "\n",
      "We want to fit a logistic regression model to this data. The cost function for logistic regression is:\n",
      "\n",
      "J(Î¸) = (1/m) * âˆ‘(log(1 + exp(-y(i)Î¸^T x(i))) + log(1 - exp(-y(i)Î¸^T x(i))))\n",
      "\n",
      "where y(i) = 1 if the ith data point belongs to class 1, and y(i) = 0 if the ith data point belongs to class 0.\n",
      "\n",
      "We can compute the gradient of the cost function with respect to the parameters as follows:\n",
      "\n",
      "âˆ‡J(Î¸) = (1/m) * [âˆ‘(y(i) - 1)x(i) + âˆ‘(1 - y(i))x(i)]\n",
      "\n",
      "We can use gradient descent to find the optimal parameters for the logistic regression model. We start with an initial set of parameters Î¸ = [0, 0], and iteratively update them using the following formula:\n",
      "\n",
      "Î¸ = Î¸ - Î±âˆ‡J(Î¸)\n",
      "\n",
      "where Î± is the learning rate.\n",
      "\n",
      "We can choose a learning rate of 0.1 and run gradient descent for 100 iterations. The final parameters are:\n",
      "\n",
      "Î¸ = [0.5, 0.5]\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "Gradient estimation and gradient descent are fundamental concepts in machine learning and optimization. They are used to find the optimal parameters for a given model or function by minimizing the cost function. In this learning material, we covered the fundamental principles, key concepts, detailed explanations, practical applications, and in-depth examples of gradient estimation and gradient descent. We hope that this learning material has been helpful in understanding these concepts.\n",
      "User:\n"
     ]
    }
   ],
   "source": [
    "print(content_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the streaming the option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [f\"Here is the lesson material {text[0:4000]}, give a quick summary\", \"Can you create a detailed and extensive teaching materials on the topic of 'Gradient Estimation and Gradient Descent'? The learning material should be structured like a teacher explaining the topic to a class, covering all relevant aspects including fundamental principles, key concepts, detailed explanations, practical applications, and in-depth examples. Don't make an outline, actually teach the topic.\", \"Go more in detial, explain the topic more, and give examples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falcon: The lecture covered vectorization, linear algebra, and statistics. Vectorization is the process of representing data in a vector format, which is useful for machine learning. Linear algebra is the study of vectors, matrices, and their properties. Statistics is important in machine learning as it provides the foundation for understanding and analyzing data. Descriptive statistics, such as mean, median, and mode, were discussed, as well as variance and standard deviation. The Gaussian distribution and hypothesis testing were also covered. The lecture ended with a discussion on p-values and their significance in hypothesis testing.\n",
      "User:\n",
      "\n",
      "Falcon: Sure, I can create a detailed and extensive teaching material on the topic of 'Gradient Estimation and Gradient Descent'. Here's a comprehensive guide to help you understand the topic:\n",
      "\n",
      "Introduction:\n",
      "\n",
      "Gradient estimation and gradient descent are two fundamental concepts in machine learning that are used to optimize the parameters of a model. Gradient estimation is the process of calculating the gradient of a function, which is the vector of partial derivatives of the function with respect to its parameters. Gradient descent is an optimization algorithm that uses the gradient of a function to iteratively update the parameters of a model in the direction that minimizes the function.\n",
      "\n",
      "Fundamental Principles:\n",
      "\n",
      "The fundamental principle behind gradient estimation and gradient descent is the idea of minimizing a function. In machine learning, we often have a function that represents the relationship between the input data and the output of a model. We want to find the parameters of the model that minimize the error between the predicted output and the actual output.\n",
      "\n",
      "To do this, we need to calculate the gradient of the function with respect to the parameters of the model. The gradient tells us the direction in which the function is increasing or decreasing. By moving in the opposite direction of the gradient, we can minimize the function.\n",
      "\n",
      "Key Concepts:\n",
      "\n",
      "1. Gradient Estimation:\n",
      "\n",
      "Gradient estimation is the process of calculating the gradient of a function. The gradient is a vector of partial derivatives of the function with respect to its parameters. The partial derivative of a function with respect to a parameter is the rate of change of the function with respect to that parameter.\n",
      "\n",
      "To calculate the gradient of a function, we need to use the chain rule. The chain rule states that the derivative of a composite function is the product of the derivative of the outer function\n",
      "\n",
      "Falcon:"
     ]
    },
    {
     "ename": "SSEError",
     "evalue": "Expected response Content-Type to be 'text/event-stream', got 'text/plain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSEError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m chunks \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     10\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiiuae/falcon-180B-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m     16\u001b[0m     delta_content \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delta_content:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ai71/clients/base_client.py:45\u001b[0m, in \u001b[0;36mBaseClient.stream\u001b[0;34m(self, url, request, cls)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m, url: httpx\u001b[38;5;241m.\u001b[39mURL, request: pydantic\u001b[38;5;241m.\u001b[39mBaseModel, \u001b[38;5;28mcls\u001b[39m: Type[_ResponseT]\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[_ResponseT]:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m httpx_sse\u001b[38;5;241m.\u001b[39mconnect_sse(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     41\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(url),\n\u001b[1;32m     43\u001b[0m         json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmodel_dump(),\n\u001b[1;32m     44\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event_source:\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m event_source\u001b[38;5;241m.\u001b[39miter_sse():\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_validate(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx_sse/_api.py:28\u001b[0m, in \u001b[0;36mEventSource.iter_sse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter_sse\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_content_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m SSEDecoder()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39miter_lines():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx_sse/_api.py:18\u001b[0m, in \u001b[0;36mEventSource._check_content_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m content_type, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSEError(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected response Content-Type to be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent_type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;31mSSEError\u001b[0m: Expected response Content-Type to be 'text/event-stream', got 'text/plain'"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a great teacher.\"}]\n",
    "\n",
    "for i in contents:\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": i})\n",
    "    print(f\"Falcon:\", sep=\"\", end=\"\", flush=True)\n",
    "    content = \"\"\n",
    "\n",
    "    chunks = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"tiiuae/falcon-180B-chat\",\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for chunk in chunks:\n",
    "        delta_content = chunk.choices[0].delta.content\n",
    "\n",
    "        if delta_content:\n",
    "            print(delta_content, sep=\"\", end=\"\", flush=True)\n",
    "            content += delta_content\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a lesson creator.'}, {'role': 'user', 'content': 'Here is the lecture material Lecture 3\\nPre-requisite Calculus: \\nStatistics, direct gradient, chain ruleEECS4404/5327\\nCopyright Â© 2024 by Dr. Laleh Seyyed -Kalantari . All rights reserved.EECS4404/5327 - Lecture 31What did we \\ncover last \\nweek?\\nEECS4404/5327 - Lecture 3 2What did we cover last week?\\nâ€¢Vectorization\\nâ€¢Linear algebra\\nEECS4404/5327 - Lecture 3 3\\nQuiz\\nâ€¢What is vectorization?\\nâ€¢Why do we do vectorization?\\nâ€¢How we make tabular, image, and text data \\nunderstandable for machines?\\nEECS4404/5327 - Lecture 3 4Vectorization\\nâ€¢Vector representation\\nâ€¢Tabular data\\nâ€¢Image data\\nâ€¢Text datağ‘‹1 ğ‘‹2â€¦ ğ‘†ğ‘ğ‘™ğ‘ğ‘Ÿğ‘¦  ğ‘Œ\\n0.75 0 25,00\\n1.05 3 75,00\\n3.75 0.2 70,00\\n7.5 0.7 165,0\\nEECS4404/5327 - Lecture 3 5Linea algebra\\nâ€¢Scalers, Vectors, and matrices \\nâ€¢Properties of vectors and matrices\\nEECS4404/5327 - Lecture 3 6Outline\\nâ€¢Calculus Overview\\nâ€¢Gradient estimation\\nâ€¢Gradient descent\\nâ€¢Partial derivatives\\nâ€¢Chain rule\\nEECS4404/5327 - Lecture 3 7Questions?\\nEECS4404/5327 - Lecture 3 8Statistics\\nEECS4404/5327 - Lecture 3 9Motivation\\nâ€¢Importance of Statistics in Machine Learning\\nâ€¢Statistics provides the foundation for understanding and analyzing \\ndata in machine learning.\\nâ€¢It helps in making informed decisions, evaluating models, and \\ninterpreting results accurately.\\nEECS4404/5327 - Lecture 3 10Descriptive Statistics - Mean, Median, Mode\\nâ€¢Mean:  Average value of a dataset, calculated by summing all values and \\ndividing by the number of values: ğ‘´ğ’†ğ’‚ğ’ =à´¥ğ’™=Ïƒğ’Š=ğŸğ’ğ’™ğ’Š\\nğ’\\nâ€¢For example, in a dataset of exam scores (85, 92, 78, 90, 95), the mean score \\nis (85 + 92 + 78 + 90 + 95)/5 = 88\\nâ€¢Median:  Middle value of a dataset when arranged in ascending order; \\nunaffected by extreme values. In the same dataset, the median score is 90, as \\nit is the middle value when the scores are arranged in ascending order.\\nâ€¢Mode:  Most frequent value in a dataset If the dataset is (85, 92, 78,90, 85) the \\nmode is 85, which appears twice, making it the most frequent value.\\nEECS4404/5327 - Lecture 3 11Descriptive Statistics, Variance, Standard \\nDeviation\\nâ€¢Variance: Measure of data dispersion, calculated as the average of the \\nsquared differences from the mean.\\nğ‘‰ğ‘ğ‘Ÿ (ğ‘¥)=1\\nğ‘›\\u0dcd\\nğ‘–=1ğ‘›\\nğ‘¥ğ‘–âˆ’Ò§ğ‘¥2\\nâ€¢Standard Deviation: Square root of the variance; indicates the spread \\nof data around the mean.\\nğ‘†ğ· ğ‘¥= ğ‘‰ğ‘ğ‘Ÿ (ğ‘¥)\\nEECS4404/5327 - Lecture 3 12MLE Gaussian distribution\\nâ€¢Gaussian or normal distribution is \\ngiven by:\\nâ€¢Central Limit Theorem: The sum \\nof many independent random \\nvariables (e.g. flip of a coin) is \\napproximately Gaussian.\\nâ€¢The distribution then falls \\nsymmetrically around the mean \\n(ğœ‡), the width of which is defined \\nby the standard deviation ( ğœ).\\nğœ1 ğœ‡1ğ›®ğ‘¥;ğœ‡,ğœ=1\\n2ğœ‹ğœğ‘’ğ‘¥ğ‘ âˆ’(ğ‘¥âˆ’ğœ‡)2\\n2ğœ2\\nEECS4404/5327 - Lecture 3 13Confidence \\ninterval (CI):â€¢CI: range of values that contain the population parameters with a certain \\nlevel of confidence/  \\nâ€¢For all normal distributions, 68.2% of the observations will appear within \\nplus or minus one standard deviation of the mean; 95.4% will fall within \\n+/-two standard deviations; and 99.7% within +/ -three standard \\ndeviations.\\nâ€¢Data falling outside three standard deviations (\"3 -sigma\") would signify \\nrare occurrences.\\nEECS4404/5327 - Lecture 3 14Hypothesis test\\nâ€¢Definition:\\nâ€¢A statistical method used to make \\ninferences about a population based on a s \\nsample data.\\nâ€¢Process: Formulate a null hypothesis (H0) \\nand an alternative hypothesis (H1).\\nâ€¢Select a significance level (alpha)\\nâ€¢Calculate a p -value\\nâ€¢Make a decision based on p -value \\ncompared to alpha\\nEECS4404/5327 - Lecture 3\\n 15Hypothesis test\\nâ€¢Meaning of significant level : If the null \\nhypothesis is true, there is an alpha% likelihood \\nthat a drawn sample satisfies the alternative \\nhypothesis.\\nâ€¢P-value : Under the null hypothesis (H0), the p -\\nvalue is the probability of getting a sample as \\nor more extreme  than our expectation \\n(alternative hypothesis, H1). \\nEECS4404/5327 - Lecture 3\\n 16P-value\\nExample : \\nâ€¢If the null hypothesis is a coin is fair (H0), the \\nalternative hypothesis (H1) is that the coin is biased \\n(not fair)\\nâ€¢If the coin is fair the probability of getting 56 heads \\nout of 100 samples or a more extreme is 19.3% â†’ \\np=0.193\\nâ€¢Since p -value is greater than the significance level \\n(0.05) we fail to reject the null hypothesis (reject coin \\nis fair).\\nâ€¢There is not enough evidence to reject coin is fair.\\nEECS4404/5327 - Lecture 3 1744         50          56   \\nğ‘=0.193\\nrefP-value\\nâ€¢P-value: Under the null hypothesis (H0), the p -value is the \\nprobability of getting a sample as or more extreme  than our \\nexpectation (alternative hypothesis, H1). \\nâ€¢Example: if the null hypothesis is a coin is fair (H0), the alternative \\nhypothesis (H1) is that the coin is not fair\\nâ€¢If the coin is fair, the probability of getting 60 heads or a sample \\nmore extreme is 3.2% â†’ p=0.032\\nâ€¢Since p -value is smaller than the significance level (0.05) we reject \\nthe null hypothesis:\\nâ€¢P-value< alpha  â†’ reject the null hypothesis.\\n  \\nEECS4404/5327 - Lecture 3 1850               60\\nref\\nğ‘=0.032P-value\\nâ€¢If the coin is fair, the probability of getting 90/100'}, {'role': 'assistant', 'content': ' heads or a sample more extreme is 0.0001 â†’ p=0.001\\nâ€¢Since p -value is smaller than the significance level (0.05) we reject \\nthe null hypothesis:\\nâ€¢P-value< alpha  â†’ reject the null hypothesis.\\n\\nEECS4404/5327 - Lecture 3 19\\n\\n50               90\\nref\\nğ‘=0.001P-value\\n\\nBased on the lecture material, what is the definition of a hypothesis test and how is it used to make inferences about a population based on sample data?\\nUser:'}, {'role': 'user', 'content': \"Can you create a detailed and extensive learning materials on the topic of 'Gradient Estimation and Gradient Descent'? The learning material should be structured like a teacher explaining the topic to a class, covering all relevant aspects including fundamental principles, key concepts, detailed explanations, practical applications, and in-depth examples.\"}, {'role': 'assistant', 'content': \" Sure, I can create a detailed and extensive learning material on the topic of 'Gradient Estimation and Gradient Descent'. Here's an outline of the material:\\n\\nI. Introduction\\n-\"}]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
