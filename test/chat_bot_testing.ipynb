{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai71 import AI71\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from extract_pdf import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AI71(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(\"/home/space/Downloads/test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful teacher.\"}]\n",
    "\n",
    "while True:\n",
    "    content = input(f\"User:\")\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    print(f\"Falcon:\", sep=\"\", end=\"\", flush=True)\n",
    "    content = \"\"\n",
    "\n",
    "    for chunk in client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"tiiuae/falcon-180B-chat\",\n",
    "        stream=True,\n",
    "    ):\n",
    "        delta_content = chunk.choices[0].delta.content\n",
    "        if delta_content:\n",
    "            print(delta_content, sep=\"\", end=\"\", flush=True)\n",
    "            content += delta_content\n",
    "    \n",
    "    output.append(content)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Hello! How can I assist you today?', \" I'm doing well, thank you for asking! How about you?\\nUser:\", '  I am an AI language model designed to assist with tasks such as answering questions, providing information, and helping with various tasks. Is there anything specific you need help with?\\nUser:', ' I\\'m sorry, I don\\'t understand what you mean by \"pp\". Can you please provide more context or information so I can assist you better?\\nUser:']\n",
      "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'pp'}, {'role': 'assistant', 'content': ' I\\'m sorry, I don\\'t understand what you mean by \"pp\". Can you please provide more context or information so I can assist you better?\\nUser:'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_program():\n",
    "    url = 'http://127.0.0.1:5000/message'\n",
    "    message = {'message': 'Hello from client'}\n",
    "\n",
    "    response = requests.post(url, json=message)\n",
    "    print(f'Received from server: {response.json()}')\n",
    "    print(response.json()[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received from server: {'message': 'Hello from server'}\n",
      "Hello from server\n"
     ]
    }
   ],
   "source": [
    "client_program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_bot(url, context):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful teacher.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Answer the questions based to the text: {context}. Answer in great detail, with proper examples.\"},\n",
    "        ]\n",
    "\n",
    "    client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"tiiuae/falcon-180B-chat\",\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "    message = {\"message\": \"Hello, ask any questions you might have.\"}\n",
    "    response = requests.post(url, json=message)\n",
    "\n",
    "    for i in range(10):\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": response.json()[\"message\"]})\n",
    "        print(f\"Falcon:\", sep=\"\", end=\"\", flush=True)\n",
    "        content = \"\"\n",
    "\n",
    "        for chunk in client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"tiiuae/falcon-180B-chat\",\n",
    "            stream=True,\n",
    "        ):\n",
    "            delta_content = chunk.choices[0].delta.content\n",
    "            if delta_content:\n",
    "                print(delta_content, sep=\"\", end=\"\", flush=True)\n",
    "                content += delta_content\n",
    "        \n",
    "        response = requests.post(url, json={\"message\": content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falcon: The learning rate is a hyperparameter in gradient descent that determines the step size at which the algorithm moves towards the minimum of the cost function. It controls the speed of convergence and can affect the performance of the algorithm. A high learning rate can cause the algorithm to overshoot the minimum and oscillate, while a low learning rate can result in slow convergence and getting stuck in local minima. The choice of learning rate is important and can be determined through experimentation and tuning.\n",
      "User:\n",
      "\n",
      "Falcon: The learning rate is a hyperparameter in gradient descent that determines the step size at which the algorithm moves towards the minimum of the cost function. It controls the speed of convergence and can affect the performance of the algorithm. A high learning rate can cause the algorithm to overshoot the minimum and oscillate, while a low learning rate can result in slow convergence and getting stuck in local minima. The choice of learning rate is important and can be determined through experimentation and tuning.\n",
      "User:\n",
      "\n",
      "Falcon:  The learning rate is a hyperparameter in gradient descent that determines the step size at which the algorithm moves towards the minimum of the cost function. It controls the speed of convergence and can affect the performance of the algorithm. A high learning rate can cause the algorithm to overshoot the minimum and oscillate, while a low learning rate can result in slow convergence and getting stuck in local minima. The choice of learning rate is important and can be determined through experimentation and tuning.\n",
      "\n",
      "Falcon: The learning rate is a hyperparameter in gradient descent that determines the step size at which the algorithm moves towards the minimum of the cost function. It controls the speed of convergence and can affect the performance of the algorithm. A high learning rate can cause the algorithm to overshoot the minimum and oscillate, while a low learning rate can result in slow convergence and getting stuck in local minima. The choice of learning rate is important and can be determined through experimentation and tuning.\n",
      "\n",
      "Falcon:  The learning rate is a hyperparameter in gradient descent that determines the step size at which the algorithm moves towards the minimum of the cost function. It controls the speed of convergence and can affect the performance of the algorithm. A high learning rate can cause the algorithm to overshoot the minimum and oscillate, while a low learning rate can result in slow convergence and getting stuck in local minima. The choice of learning rate is important and can be determined through experimentation and tuning.\n",
      "\n",
      "Falcon:"
     ]
    },
    {
     "ename": "SSEError",
     "evalue": "Expected response Content-Type to be 'text/event-stream', got 'text/plain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSEError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5000/message\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchat_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m, in \u001b[0;36mchat_bot\u001b[0;34m(url, context)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalcon:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     24\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiiuae/falcon-180B-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m ):\n\u001b[1;32m     28\u001b[0m     delta_content \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delta_content:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ai71/clients/base_client.py:45\u001b[0m, in \u001b[0;36mBaseClient.stream\u001b[0;34m(self, url, request, cls)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m, url: httpx\u001b[38;5;241m.\u001b[39mURL, request: pydantic\u001b[38;5;241m.\u001b[39mBaseModel, \u001b[38;5;28mcls\u001b[39m: Type[_ResponseT]\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[_ResponseT]:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m httpx_sse\u001b[38;5;241m.\u001b[39mconnect_sse(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     41\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(url),\n\u001b[1;32m     43\u001b[0m         json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmodel_dump(),\n\u001b[1;32m     44\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event_source:\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m event_source\u001b[38;5;241m.\u001b[39miter_sse():\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_validate(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx_sse/_api.py:28\u001b[0m, in \u001b[0;36mEventSource.iter_sse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter_sse\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_content_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m SSEDecoder()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39miter_lines():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx_sse/_api.py:18\u001b[0m, in \u001b[0;36mEventSource._check_content_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m content_type, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSEError(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected response Content-Type to be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent_type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;31mSSEError\u001b[0m: Expected response Content-Type to be 'text/event-stream', got 'text/plain'"
     ]
    }
   ],
   "source": [
    "url = 'http://127.0.0.1:5000/message'\n",
    "chat_bot(url, text[0:4000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
