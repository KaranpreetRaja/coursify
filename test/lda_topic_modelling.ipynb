{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from extract_pdf import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/space/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(\"/home/space/Downloads/test2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert text to Document-Term Matrix\n",
    "def convert_to_dtm(text_data):\n",
    "    vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    return dtm, vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform LDA Topic Modeling\n",
    "def perform_lda(dtm, num_topics=5):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    return lda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Interpret the Results\n",
    "def print_topics(model, vectorizer, num_words=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic #{topic_idx + 1}:\")\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-num_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_topics=5, num_words=50):\n",
    "    text = extract_text_from_pdf(\"/home/space/Downloads/test2.pdf\")\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    dtm, vectorizer = convert_to_dtm([preprocessed_text])\n",
    "    lda_model = perform_lda(dtm, num_topics)\n",
    "    print_topics(lda_model, vectorizer, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "rescue lake elliot emergency operation time incident staff response insp evidence plan neadles collapse team action chief ucrt member responder commission communication effort command information organization commander event general inquiry planning decision deployment department officer capability crane medical task training experience opp left expertise ministry section heard ontario june clearly\n",
      "Topic #2:\n",
      "winter ability absence year wrong absent writing worthwhile woman affected advisory worker workplace whereabouts whim adoption algo albeit aimed world achievable accumulates accumulated accountability correct accordingly accorded accident accessible controlled courage correctional contacted coroner cordon copy coping convoy trained tragedy tower troop trite took tony told today timmins timing timely\n",
      "Topic #3:\n",
      "winter ability absence year wrong absent writing worthwhile woman affected advisory worker workplace whereabouts whim adoption algo albeit aimed world achievable accumulates accumulated accountability correct accordingly accorded accident accessible controlled courage correctional contacted coroner cordon copy coping convoy trained tragedy tower troop trite took tony told today timmins timing timely\n",
      "Topic #4:\n",
      "winter ability absence year wrong absent writing worthwhile woman affected advisory worker workplace whereabouts whim adoption algo albeit aimed world achievable accumulates accumulated accountability correct accordingly accorded accident accessible controlled courage correctional contacted coroner cordon copy coping convoy trained tragedy tower troop trite took tony told today timmins timing timely\n",
      "Topic #5:\n",
      "winter ability absence year wrong absent writing worthwhile woman affected advisory worker workplace whereabouts whim adoption algo albeit aimed world achievable accumulates accumulated accountability correct accordingly accorded accident accessible controlled courage correctional contacted coroner cordon copy coping convoy trained tragedy tower troop trite took tony told today timmins timing timely\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
